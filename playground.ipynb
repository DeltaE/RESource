{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workflow.scripts.resources as res_module\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import linkingtool.utility as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Store initialized with the given path: data/store/resources_BC.h5\n"
     ]
    }
   ],
   "source": [
    "from linkingtool.hdf5_handler import DataHandler\n",
    "import linkingtool.visuals as vis\n",
    "store=f\"data/store/resources_BC.h5\"\n",
    "bccm_store=f\"/localhome/mei3/eliasinul/work/BC_Combined_Modelling/data/store/resources_BC.h5\"\n",
    "datahandler=DataHandler(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of HDF5 file: /localhome/mei3/eliasinul/work/BC_Combined_Modelling/data/store/resources_BC.h5\n",
      "[Group] boundary\n",
      "[Group] cells\n",
      "[Group] clusters\n",
      "  └─ [Group] clusters/solar\n",
      "  └─ [Group] clusters/wind\n",
      "[Group] dissolved_indices\n",
      "  └─ [Group] dissolved_indices/solar\n",
      "  └─ [Group] dissolved_indices/wind\n",
      "[Group] substations\n",
      "[Group] timeseries\n",
      "  └─ [Group] timeseries/clusters\n",
      "  └─   └─ [Group] timeseries/clusters/solar\n",
      "  └─   └─ [Group] timeseries/clusters/wind\n",
      "  └─ [Group] timeseries/solar\n",
      "  └─ [Group] timeseries/wind\n",
      "[Group] units\n"
     ]
    }
   ],
   "source": [
    "datahandler.show_tree(bccm_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "``/local-scratch/localhome/mei3/eliasinul/work/BC_Combined_Modelling/models/Linking_tool/data/store/resources_BC.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sites\u001b[38;5;241m=\u001b[39m\u001b[43mdatahandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_store\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclusters/solar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local-scratch/localhome/mei3/eliasinul/work/BC_Combined_Modelling/models/Linking_tool/linkingtool/hdf5_handler.py:86\u001b[0m, in \u001b[0;36mDataHandler.from_store\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_store\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     78\u001b[0m                key: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Load data from the HDF5 store and handle geometry conversion.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    :param key: Key for loading the DataFrame or GeoDataFrame.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    :return: DataFrame or GeoDataFrame based on the data loaded.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m store:\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/pandas/io/pytables.py:579\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fletcher32 \u001b[38;5;241m=\u001b[39m fletcher32\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/pandas/io/pytables.py:731\u001b[0m, in \u001b[0;36mHDFStore.open\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open HDF5 file, which is already opened, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven in read-only mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/tables/file.py:296\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already opened.  Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose it before reopening in write mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/tables/file.py:746\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_new\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/tables/hdf5extension.pyx:396\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/tables/utils.py:166\u001b[0m, in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# The file should be readable.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(path, os\u001b[38;5;241m.\u001b[39mF_OK):\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`` is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ``/local-scratch/localhome/mei3/eliasinul/work/BC_Combined_Modelling/models/Linking_tool/data/store/resources_BC.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "sites=datahandler.from_store('clusters/solar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_sites(\n",
    "    all_scored_sites_gdf:gpd.GeoDataFrame, \n",
    "    resource_max_capacity:float)-> gpd.GeoDataFrame:\n",
    "    print(f\">>> Selecting TOP Sites to for {resource_max_capacity} GW Capacity Investment in BC...\")\n",
    "    \"\"\"\n",
    "    Select the top sites based on potential capacity and a maximum resource capacity limit.\n",
    "\n",
    "    Parameters:\n",
    "    - sites_gdf: GeoDataFrame containing  cell and bucket information.\n",
    "    - resource_max_capacity : Maximum allowable  capacity in GW.\n",
    "\n",
    "    Returns:\n",
    "    - selected_sites: GeoDataFrame with the selected top sites.\n",
    "    \"\"\"\n",
    "    print(f\"{'_'*50}\")\n",
    "    print(f\"Selecting the Top Ranked Sites to invest in {resource_max_capacity} GW PV in BC\")\n",
    "    print(f\"{'_'*50}\\n\")\n",
    "\n",
    "    # Initialize variables\n",
    "    selected_rows:list = []\n",
    "    total_capacity:float = 0.0\n",
    "\n",
    "    top_sites:gpd.GeoDataFrame = all_scored_sites_gdf.copy()\n",
    "\n",
    "    if top_sites['potential_capacity'].iloc[0] < resource_max_capacity * 1000:\n",
    "        # Iterate through the sorted GeoDataFrame\n",
    "        for index, row in top_sites.iterrows():\n",
    "            # Check if adding the current row's capacity exceeds resource capacity\n",
    "            if total_capacity + row['potential_capacity'] <= resource_max_capacity * 1000:\n",
    "                selected_rows.append(index)  # Add the row to the selection\n",
    "                # Update the total capacity\n",
    "                total_capacity += row['potential_capacity']\n",
    "            # If adding the current row's capacity would exceed max resource capacity, stop the loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Create a new GeoDataFrame with the selected rows\n",
    "        top_sites:gpd.GeoDataFrame = top_sites.loc[selected_rows]\n",
    "\n",
    "        # Apply the additional logic\n",
    "        mask = all_scored_sites_gdf['Site_ID'] > top_sites['Site_ID'].max()\n",
    "        selected_additional_sites:gpd.GeoDataFrame = all_scored_sites_gdf[mask].head(1)\n",
    "        \n",
    "        remaining_capacity:float = resource_max_capacity * 1000 - top_sites['potential_capacity'].sum()\n",
    "\n",
    "        if remaining_capacity > 0:\n",
    "            \n",
    "            # selected_additional_sites['capex'] = capex* remaining_capacity\n",
    "            print(f\"\\n!! Note: The Last cluster originally had {round(selected_additional_sites['potential_capacity'].iloc[0] / 1000,2)} GW potential capacity.\"\n",
    "                 f\"To fit the maximum capacity investment of {resource_max_capacity} GW, it has been adjusted to {round(remaining_capacity / 1000,2)} GW\\n\")\n",
    "            \n",
    "            selected_additional_sites['potential_capacity'] = remaining_capacity\n",
    "        # Concatenate the DataFrames\n",
    "        top_sites = pd.concat([top_sites, selected_additional_sites])\n",
    "    else:\n",
    "        original_capacity = all_scored_sites_gdf['potential_capacity'].iloc[0]\n",
    "\n",
    "        print(f\"\\n!! Note: The first cluster originally had {round(original_capacity / 1000,2)} GW potential capacity.\"\n",
    "              f\"To fit the maximum capacity investment of {resource_max_capacity} GW, it has been adjusted. \\n\")\n",
    "\n",
    "        top_sites = top_sites.iloc[:1]  # Keep only the first row\n",
    "        # Adjust the potential_capacity of the first row\n",
    "        top_sites.at[top_sites.index[0], 'potential_capacity'] = resource_max_capacity * 1000\n",
    "\n",
    "    return top_sites  # gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Required Args to Activate Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Store initialized with the given path: data/store/resources_BC.h5\n",
      ">> Store initialized with the given path: data/store/resources_BC.h5\n",
      ">> Store initialized with the given path: data/store/resources_BC.h5\n",
      ">> Store initialized with the given path: data/store/resources_BC.h5\n",
      ">> Store initialized with the given path: data/store/resources_BC.h5\n",
      ">> Store initialized with the given path: data/store/resources_BC.h5\n"
     ]
    }
   ],
   "source": [
    "# Iterate over provinces for both solar and wind resources\n",
    "resource_types = ['wind'] # , 'solar'\n",
    "provinces=['BC']  #'AB','SK','ON','NS'\n",
    "for province_code in provinces:\n",
    "    for resource_type in resource_types:\n",
    "        required_args = {\n",
    "            \"config_file_path\": 'config/config.yaml',\n",
    "            \"province_short_code\": province_code,\n",
    "            \"resource_type\": resource_type\n",
    "        }\n",
    "        \n",
    "        # Create an instance of Resources and execute the module\n",
    "        resources = res_module.Resources(**required_args)\n",
    "        # resource_module.execute_module()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 15:02:14,657 - INFO - >> Data pulled substations from [source checked: CODERS(https://sesit.dev/api/docs)]\n",
      "2024-11-28 15:02:14,660 - INFO - substations data saved to:\n",
      " data/downloaded_data/CODERS/network/substations.pkl\n",
      "2024-11-28 15:02:14,688 - INFO - >> Loading GADM boundaries (Sub-provincial | level =2) for British Columbia  from local file data/processed_data/regions/gadm41_Canada_L2_BC.geojson.\n",
      "2024-11-28 15:02:14,801 - INFO - Building new cutout data/downloaded_data/cutout/BC_2023_2024.nc\n",
      "2024-11-28 15:02:14,833 - INFO - Storing temporary files in /tmp/tmpn79hqhwd\n",
      "2024-11-28 15:02:14,836 - INFO - Calculating and writing with module era5:\n",
      "2024-11-28 15:02:14,839 - INFO - Requesting data for feature influx...\n",
      "2024-11-28 15:02:14,841 - INFO - Requesting data for feature temperature...\n",
      "2024-11-28 15:02:14,842 - INFO - Requesting data for feature wind...\n",
      "2024-11-28 15:02:14,843 - INFO - Requesting data for feature runoff...\n",
      "2024-11-28 15:02:14,845 - INFO - Requesting data for feature height...\n",
      "2024-11-28 15:02:16,570 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,570 - WARNING - [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,571 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:16,571 - WARNING - [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:16,638 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,638 - WARNING - [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,639 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:16,639 - WARNING - [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:16,722 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,722 - WARNING - [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:16,723 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:16,723 - WARNING - [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:17,678 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:17,678 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:17,834 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:17,834 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:17,842 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:17,842 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:21,233 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:21,233 - WARNING - [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:21,234 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:21,234 - WARNING - [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:21,237 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:21,237 - WARNING - [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-28 15:02:21,237 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:21,237 - WARNING - [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-28 15:02:22,097 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:22,097 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:22,161 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-28 15:02:22,161 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_grid_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local-scratch/localhome/mei3/eliasinul/work/BC_Combined_Modelling/models/Linking_tool/workflow/scripts/resources.py:136\u001b[0m, in \u001b[0;36mResources.find_grid_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m=\u001b[39mGridNodeLocator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_args)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pypsa_buses:\n\u001b[0;32m--> 136\u001b[0m     buses_data_path\u001b[38;5;241m=\u001b[39mPath (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpypsa\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepare_base_network\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuses.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m     grid_ss_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(buses_data_path)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_ss \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(\n\u001b[1;32m    139\u001b[0m         geometry\u001b[38;5;241m=\u001b[39mgpd\u001b[38;5;241m.\u001b[39mpoints_from_xy(grid_ss_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], grid_ss_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    140\u001b[0m         crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_crs(),  \u001b[38;5;66;03m# Set the coordinate reference system (e.g., WGS84)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         ) \n",
      "File \u001b[0;32m/local-scratch/localhome/mei3/eliasinul/work/BC_Combined_Modelling/models/Linking_tool/linkingtool/era5_cutout.py:90\u001b[0m, in \u001b[0;36mERA5Cutout.get_era5_cutout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Create the cutout based on bounds found from above\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     cutout \u001b[38;5;241m=\u001b[39m atlite\u001b[38;5;241m.\u001b[39mCutout(\n\u001b[1;32m     81\u001b[0m         path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcutout_path,\n\u001b[1;32m     82\u001b[0m         module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcutout_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         time\u001b[38;5;241m=\u001b[39mtime_horizon\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mcutout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Prepare the cutout data\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m>>> Memory management remarks:\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124m* After execution, all downloaded data is stored at cutout.path. By default, it is not loaded into memory, but into dask arrays. This keeps the memory consumption extremely low.\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m* The data is accessible in cutout.data, which is an xarray.Dataset. Querying the cutout gives us some basic information on which data is contained in it.\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124m* For more operations related to cutout, check the tool docs @ https://atlite.readthedocs.io/en/master/examples/create_cutout.html#\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     cutout,province_boundary\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/atlite/data.py:116\u001b[0m, in \u001b[0;36mmaybe_remove_tmpdir.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmpdir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mkdtemp()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     rmtree(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmpdir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/atlite/data.py:208\u001b[0m, in \u001b[0;36mcutout_prepare\u001b[0;34m(cutout, features, tmpdir, overwrite, compression, show_progress, dask_kwargs, monthly_requests, concurrent_requests)\u001b[0m\n\u001b[1;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating and writing with module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m missing_features \u001b[38;5;241m=\u001b[39m missing_vars\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39munique(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcutout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonthly_requests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonthly_requests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrent_requests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent_requests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m prepared \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(missing_features)\n\u001b[1;32m    218\u001b[0m cutout\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(prepared_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(prepared)))\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/atlite/data.py:59\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(cutout, module, features, tmpdir, monthly_requests, concurrent_requests)\u001b[0m\n\u001b[1;32m     48\u001b[0m     feature_data \u001b[38;5;241m=\u001b[39m delayed(get_data)(\n\u001b[1;32m     49\u001b[0m         cutout,\n\u001b[1;32m     50\u001b[0m         feature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mappend(feature_data)\n\u001b[0;32m---> 59\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mmerge(datasets, compat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m ds:\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bc_combined_modelling/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resources.find_grid_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 12:43:25,500 - INFO - >> Preparing spatial clusters for 2956 Cells\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Calculating Score for each Cell...\n",
      ">> Updated 'cells' saved to data/store/resources_BC.h5 with key 'cells'\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Stikine - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Columbia-Shuswap - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Kitimat-Stikine - Optimal k for lcoe_wind based clustering: 5\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone CentralCoast - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone CentralKootenay - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Comox-Strathcona - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Cariboo - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone MountWaddington - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone NorthOkanagan - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone PowellRiver - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone SunshineCoast - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Squamish-Lillooet - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Thompson-Nicola - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone PeaceRiver - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone EastKootenay - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Fraser-FortGeorge - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone FraserValley - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Bulkley-Nechako - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone NorthernRockies - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone KootenayBoundary - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Skeena-QueenCharlotte - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone GreaterVancouver - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Alberni-Clayoquot - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Okanagan-Similkameen - Optimal k for lcoe_wind based clustering: 4\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone CentralOkanagan - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone CowichanValley - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Nanaimo - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">> Estimating optimal number of Clusters for each region based on the Score for each Cell ...\n",
      "Zone Capital - Optimal k for lcoe_wind based clustering: 3\n",
      "\n",
      ">>> K-means clustering Elbow plots generated for each region based on the Score for each Cell ...\n",
      "Optimal-k based on 'lcoe' clustering calculated for 28 zones and saved to cell dataframe.\n",
      "\n",
      ">>> Mapping the Optimal Number of Clusters for Each region ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 12:43:29,280 - INFO -  Preparing Clusters...\n",
      "2024-11-28 12:43:29,280 - INFO -  Creating cluster for Stikine 1/28\n",
      "2024-11-28 12:43:29,299 - INFO -  Creating cluster for Columbia-Shuswap 2/28\n",
      "2024-11-28 12:43:29,314 - INFO -  Creating cluster for Kitimat-Stikine 3/28\n",
      "2024-11-28 12:43:29,366 - INFO -  Creating cluster for CentralCoast 4/28\n",
      "2024-11-28 12:43:29,416 - INFO -  Creating cluster for CentralKootenay 5/28\n",
      "2024-11-28 12:43:29,436 - INFO -  Creating cluster for Comox-Strathcona 6/28\n",
      "2024-11-28 12:43:29,480 - INFO -  Creating cluster for Cariboo 7/28\n",
      "2024-11-28 12:43:29,502 - INFO -  Creating cluster for MountWaddington 8/28\n",
      "2024-11-28 12:43:29,552 - INFO -  Creating cluster for NorthOkanagan 9/28\n",
      "2024-11-28 12:43:29,568 - INFO -  Creating cluster for PowellRiver 10/28\n",
      "2024-11-28 12:43:29,585 - INFO -  Creating cluster for SunshineCoast 11/28\n",
      "2024-11-28 12:43:29,603 - INFO -  Creating cluster for Squamish-Lillooet 12/28\n",
      "2024-11-28 12:43:29,619 - INFO -  Creating cluster for Thompson-Nicola 13/28\n",
      "2024-11-28 12:43:29,641 - INFO -  Creating cluster for PeaceRiver 14/28\n",
      "2024-11-28 12:43:29,665 - INFO -  Creating cluster for EastKootenay 15/28\n",
      "2024-11-28 12:43:29,685 - INFO -  Creating cluster for Fraser-FortGeorge 16/28\n",
      "2024-11-28 12:43:29,708 - INFO -  Creating cluster for FraserValley 17/28\n",
      "2024-11-28 12:43:29,729 - INFO -  Creating cluster for Bulkley-Nechako 18/28\n",
      "2024-11-28 12:43:29,753 - INFO -  Creating cluster for NorthernRockies 19/28\n",
      "2024-11-28 12:43:29,776 - INFO -  Creating cluster for KootenayBoundary 20/28\n",
      "2024-11-28 12:43:29,797 - INFO -  Creating cluster for Skeena-QueenCharlotte 21/28\n",
      "2024-11-28 12:43:29,878 - INFO -  Creating cluster for GreaterVancouver 22/28\n",
      "2024-11-28 12:43:29,899 - INFO -  Creating cluster for Alberni-Clayoquot 23/28\n",
      "2024-11-28 12:43:29,934 - INFO -  Creating cluster for Okanagan-Similkameen 24/28\n",
      "2024-11-28 12:43:29,957 - INFO -  Creating cluster for CentralOkanagan 25/28\n",
      "2024-11-28 12:43:29,977 - INFO -  Creating cluster for CowichanValley 26/28\n",
      "2024-11-28 12:43:29,999 - INFO -  Creating cluster for Nanaimo 27/28\n",
      "2024-11-28 12:43:30,022 - INFO -  Creating cluster for Capital 28/28\n",
      "2024-11-28 12:43:30,046 - INFO -  Culsters Created and a list generated to map the Cells inside each Cluster...\n",
      "2024-11-28 12:43:30,129 - INFO - >> Preparing representative profiles for 105 clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Data (GeoDataFrame/DataFrame) saved to data/store/resources_BC.h5 with key 'clusters/wind'\n",
      ">> Data (GeoDataFrame/DataFrame) saved to data/store/resources_BC.h5 with key 'dissolved_indices/wind'\n",
      ">> Data (GeoDataFrame/DataFrame) saved to data/store/resources_BC.h5 with key 'timeseries/clusters/wind'\n",
      "solar clusters exported to :results\n"
     ]
    }
   ],
   "source": [
    "res_module.export_results('solar',\n",
    "                          resources.get_clusters().clusters,\n",
    "                          resources.get_cluster_timeseries())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bc_combined_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
